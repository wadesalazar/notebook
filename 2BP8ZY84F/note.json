{
  "paragraphs": [
    {
      "title": "Load deps (only required for versions less than 0.6)",
      "text": "%dep\nz.reset\nz.addRepo(\"hbase-spark\").url(\"https://repository.apache.org/content/repositories/snapshots/\")\nz.load(\"org.apache.hbase:hbase-spark:2.0.0-SNAPSHOT\")",
      "dateUpdated": "Jun 27, 2016 1:03:31 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1465768436914_-1202630133",
      "id": "20160612-165356_324797851",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "DepInterpreter(%dep) deprecated. Remove dependencies and repositories through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Add repository through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@401c5efc\n"
      },
      "dateCreated": "Jun 12, 2016 4:53:56 PM",
      "dateStarted": "Jun 26, 2016 5:50:14 PM",
      "dateFinished": "Jun 26, 2016 5:51:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Grab a snapshot of the HBase table",
      "text": "//add the repo hbase-spark \"https://repository.apache.org/content/repositories/snapshots/\n//and the dep org.apache.hbase:hbase-spark:2.0.0-SNAPSHOT\n\nimport org.apache.spark._\nimport org.apache.spark.rdd.NewHadoopRDD\nimport org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}\nimport org.apache.hadoop.hbase.mapreduce.TableInputFormat\nimport java.nio.ByteBuffer\n\n//create a Hbase conf file and set parameters for the scan\nval conf2 \u003d HBaseConfiguration.create()\nconf2.set(TableInputFormat.INPUT_TABLE, \"timeseries\")\nconf2.set(\"hbase.zookeeper.quorum\", \"rhea.saturn\");\nconf2.setInt(\"hbase.zookeeper.property.clientPort\", 2181);\n\n\n//adjust these values to match your query\n//remember these are seconds so if you used milliseconds do so here\nconf2.set(TableInputFormat.SCAN_ROW_START, \"Simulation Examples.Functions.Ramp1-14659441700\")\nconf2.set(TableInputFormat.SCAN_ROW_STOP, \"Simulation Examples.Functions.Ramp1-1465944400\")\n\n//create a RDD from the table scan\nval hBaseRDD2 \u003d sc.newAPIHadoopRDD(conf2, classOf[org.apache.hadoop.hbase.mapreduce.TableInputFormat],\nclassOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],\nclassOf[org.apache.hadoop.hbase.client.Result])\n\n//exract and print triple\nval i2 \u003d hBaseRDD2.map(row \u003d\u003e{\n    //retrieve the latest values from each cell    \n    ((String.valueOf(row._2.getValue(\"cf\".getBytes,\"id\".getBytes()).map(_.toChar))),\n    (String.valueOf(row._2.getValue(\"cf\".getBytes,\"v\".getBytes()).map(_.toChar))),\n    (String.valueOf(row._2.getValue(\"cf\".getBytes,\"t\".getBytes()).map(_.toChar))))\n\n})",
      "dateUpdated": "Jun 27, 2016 12:59:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1465768445088_-66451307",
      "id": "20160612-165405_1734641709",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark._\nimport org.apache.spark.rdd.NewHadoopRDD\nimport org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}\nimport org.apache.hadoop.hbase.mapreduce.TableInputFormat\nimport java.nio.ByteBuffer\nconf2: org.apache.hadoop.conf.Configuration \u003d Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, hbase-default.xml, hbase-site.xml\nhBaseRDD2: org.apache.spark.rdd.RDD[(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result)] \u003d NewHadoopRDD[0] at newAPIHadoopRDD at \u003cconsole\u003e:38\ni2: org.apache.spark.rdd.RDD[(String, String, String)] \u003d MapPartitionsRDD[1] at map at \u003cconsole\u003e:40\n"
      },
      "dateCreated": "Jun 12, 2016 4:54:05 PM",
      "dateStarted": "Jun 27, 2016 12:59:08 PM",
      "dateFinished": "Jun 27, 2016 12:59:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Print a few tuples",
      "text": "i2.count\n//print each triple to the screen  \ni2.collect.take(1).foreach(ln \u003d\u003e {\n        println(ln._1 + \" \" + ln._2 + \" \" + ln._3)\n})",
      "dateUpdated": "Jun 27, 2016 1:00:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1465768484559_1395792083",
      "id": "20160612-165444_700622531",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res7: Long \u003d 19\nSimulation Examples.Functions.Ramp1 95 1465944177501\n"
      },
      "dateCreated": "Jun 12, 2016 4:54:44 PM",
      "dateStarted": "Jun 27, 2016 1:00:23 PM",
      "dateFinished": "Jun 27, 2016 1:01:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Cast to DF with the provided schema class and then register as temp table",
      "text": "// Provide a class to describe the schema of the RDD\ncase class Sample( id:String, v:String, t:String)\nval i3 \u003d i2.map(n \u003d\u003e \n    Sample(n._1,n._2,n._3)\n)\n\n// Convert RDD to a dataframe then to a table \ni3.toDF().registerTempTable(\"ts_data\")",
      "dateUpdated": "Jun 27, 2016 1:01:18 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1465768493384_1959479870",
      "id": "20160612-165453_2075621431",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class Sample\ni3: org.apache.spark.rdd.RDD[Sample] \u003d MapPartitionsRDD[2] at map at \u003cconsole\u003e:44\n"
      },
      "dateCreated": "Jun 12, 2016 4:54:53 PM",
      "dateStarted": "Jun 27, 2016 1:01:20 PM",
      "dateFinished": "Jun 27, 2016 1:02:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Query Table",
      "text": "%sql\n\nselect * from ts_data\n\nORDER BY t, id",
      "dateUpdated": "Jun 30, 2016 3:39:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "lineChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "t",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "v",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [
            {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "scatter": {
            "xAxis": {
              "name": "t",
              "index": 2.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "v",
              "index": 1.0,
              "aggr": "sum"
            },
            "group": {
              "name": "id",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1465768525627_-1409366175",
      "id": "20160612-165525_1825474479",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "id\tv\tt\nSimulation Examples.Functions.Ramp1\t95\t1465944177501\nSimulation Examples.Functions.Ramp1\t67\t1465944221118\nSimulation Examples.Functions.Ramp1\t51\t1465944231258\nSimulation Examples.Functions.Ramp1\t87\t1465944240882\nSimulation Examples.Functions.Ramp1\t75\t1465944251038\nSimulation Examples.Functions.Ramp1\t59\t1465944261177\nSimulation Examples.Functions.Ramp1\t95\t1465944270802\nSimulation Examples.Functions.Ramp1\t79\t1465944280958\nSimulation Examples.Functions.Ramp1\t67\t1465944291097\nSimulation Examples.Functions.Ramp1\t51\t1465944301237\nSimulation Examples.Functions.Ramp1\t87\t1465944310878\nSimulation Examples.Functions.Ramp1\t71\t1465944321017\nSimulation Examples.Functions.Ramp1\t59\t1465944331157\nSimulation Examples.Functions.Ramp1\t43\t1465944341297\nSimulation Examples.Functions.Ramp1\t75\t1465944350940\nSimulation Examples.Functions.Ramp1\t59\t1465944361079\nSimulation Examples.Functions.Ramp1\t47\t1465944371219\nSimulation Examples.Functions.Ramp1\t83\t1465944380860\nSimulation Examples.Functions.Ramp1\t67\t1465944390999\n"
      },
      "dateCreated": "Jun 12, 2016 4:55:25 PM",
      "dateStarted": "Jun 26, 2016 6:49:06 PM",
      "dateFinished": "Jun 26, 2016 6:49:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\nnotes:\n\nassumes there is a hbase table called timeseries\n\n",
      "dateUpdated": "Jun 26, 2016 6:17:34 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1465768653842_1102106079",
      "id": "20160612-165733_1301932104",
      "dateCreated": "Jun 12, 2016 4:57:33 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "HBase Historian Script 1",
  "id": "2BP8ZY84F",
  "lastReplName": {
    "value": "sql"
  },
  "angularObjects": {
    "2BN7JCHM6:shared_process": [],
    "2BRP9DAPW:shared_process": [],
    "2BP9XS4UT:shared_process": [],
    "2BPWHDB6M:shared_process": [],
    "2BR9YSU2N:shared_process": [],
    "2BNUVHBNW:shared_process": [],
    "2BR9Q5PX5:shared_process": [],
    "2BPAVC7GU:shared_process": [],
    "2BRHH2S6G:shared_process": [],
    "2BRPTVWH4:shared_process": [],
    "2BNE1DABD:shared_process": [],
    "2BQMN8B9Y:shared_process": [],
    "2BP4W8X9D:shared_process": [],
    "2BP9MH176:shared_process": [],
    "2BRHZ9VEH:shared_process": [],
    "2BNSRHX2C:shared_process": [],
    "2BR6Z8Y6Q:shared_process": []
  },
  "config": {},
  "info": {}
}